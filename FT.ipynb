{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1f9UbFWausD3NcG7apjSr6UZg_4JWKpcR",
      "authorship_tag": "ABX9TyOGcsI4crqHWw+9FPNEFg2m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/N23-17/1theka/blob/main/FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BvffXjUIcvLp"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# OLLAMA LLM CLIENT\n",
        "# ==============================\n",
        "\n",
        "class OllamaClient:\n",
        "    def __init__(self, model=\"llama3\"):\n",
        "        self.model = model\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"ollama\", \"run\", self.model],\n",
        "                input=prompt,\n",
        "                text=True,\n",
        "                capture_output=True,\n",
        "                check=True\n",
        "            )\n",
        "            return result.stdout.strip()\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(\"Error calling Ollama:\")\n",
        "            print(e.stderr)\n",
        "            sys.exit(1)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# STAGE 1 — FACT EXTRACTION\n",
        "# ==============================\n",
        "\n",
        "def extract_facts(text: str, mode: str, llm: OllamaClient) -> str:\n",
        "    stage1_prompt = f\"\"\"\n",
        "You are a forensic fact analyst.\n",
        "\n",
        "Your task is to extract explicit, verifiable factual atoms from the text below\n",
        "and organize them into structured categories.\n",
        "\n",
        "STRICT RULES:\n",
        "- One fact per item.\n",
        "- Each fact must be a short, self-contained sentence.\n",
        "- Do NOT interpret, explain, justify, or speculate.\n",
        "- Do NOT merge multiple facts.\n",
        "- If a fact is implied but not clearly stated, place it under Assumption.\n",
        "- If a category has no valid entries, omit it entirely.\n",
        "\n",
        "CATEGORIES:\n",
        "- Event\n",
        "- Decision\n",
        "- Timeline\n",
        "- Person / Organization\n",
        "- Number / Statistic\n",
        "- Claim / Argument\n",
        "- Risk\n",
        "- Assumption\n",
        "- Open Question / Uncertainty\n",
        "\n",
        "OUTPUT MODE: {mode.upper()}\n",
        "\n",
        "If OUTPUT MODE is TEXT:\n",
        "- Use clear section headers\n",
        "- Use bullet points\n",
        "\n",
        "If OUTPUT MODE is JSON:\n",
        "- Return a valid JSON object\n",
        "- Keys must be snake_case\n",
        "- Values must be arrays of strings\n",
        "- Do not include empty arrays\n",
        "- Do not include commentary\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\"\"\"\n",
        "    return llm.generate(stage1_prompt)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# STAGE 2 — INSIGHT GENERATION\n",
        "# ==============================\n",
        "\n",
        "def generate_insights(facts: str, llm: OllamaClient) -> str:\n",
        "    stage2_prompt = f\"\"\"\n",
        "You are a second-order thinker.\n",
        "\n",
        "You are given structured facts extracted from a text.\n",
        "Do NOT add new facts.\n",
        "\n",
        "Pay special attention to:\n",
        "- Decisions\n",
        "- Assumptions\n",
        "- Risks\n",
        "- Open Questions\n",
        "\n",
        "TASKS:\n",
        "\n",
        "1) Generate THREE non-obvious implications that logically follow\n",
        "   but are not explicitly stated.\n",
        "\n",
        "2) Convert the facts into reusable TEACHING POINTS.\n",
        "   - Express them as general principles.\n",
        "   - Reference the relevant fact category.\n",
        "\n",
        "FORMAT:\n",
        "\n",
        "Non-Obvious Implications:\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\n",
        "Teaching Points:\n",
        "- Lesson:\n",
        "  Based on:\n",
        "\n",
        "FACTS:\n",
        "{facts}\n",
        "\"\"\"\n",
        "    return llm.generate(stage2_prompt)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# MAIN CLI\n",
        "# ==============================\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Insight Distiller (Ollama Edition)\")\n",
        "    parser.add_argument(\"input\", help=\"Path to input text file\")\n",
        "    parser.add_argument(\n",
        "        \"--mode\",\n",
        "        choices=[\"text\", \"json\"],\n",
        "        default=\"text\",\n",
        "        help=\"Stage 1 output format\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model\",\n",
        "        default=\"llama3\",\n",
        "        help=\"Ollama model name (default: llama3)\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    input_path = Path(args.input)\n",
        "\n",
        "    if not input_path.exists():\n",
        "        print(\"Input file not found.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read()\n",
        "\n",
        "    llm = OllamaClient(model=args.model)\n",
        "\n",
        "    print(\"\\nRunning Stage 1 — Fact Extraction...\\n\")\n",
        "    facts = extract_facts(raw_text, args.mode, llm)\n",
        "\n",
        "    print(\"=== STAGE 1 OUTPUT ===\\n\")\n",
        "    print(facts)\n",
        "\n",
        "    print(\"\\nRunning Stage 2 — Insight Generation...\\n\")\n",
        "    insights = generate_insights(facts, llm)\n",
        "\n",
        "    print(\"=== STAGE 2 OUTPUT ===\\n\")\n",
        "    print(insights)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}